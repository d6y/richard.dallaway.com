---
layout:     post
title:      "Exploring CGP for playing Atari 2600 games"
author:     "Richard Dallaway"
published: false
---

A team from Toulouse and York have used Cartesian Genetic Programming to evolve programs to play Atari 2600 games. What I find exciting about this, compared to similar work, is the possibility of understanding the evolved code. This post explores the details of the representation used in the paper.

References and links are all at the end of this post.

<!-- break -->

# Evolved programs

The work I'm referring to is by Wilson et al. (2018), called "Evolving simple programs for playing Atari games". The basic idea is to evolve a game player by:

- having a population of floating point vectors to represent a program (the chromosome, a lot more on this later);
- evaluating the chromosome by playing an Atari game and seeing what score it gets; and
- evolving the population to improve the score.

The paper reports performance at this task which is "competitive with state of the art methods for the Atari benchmark set and require less training time."

In this post I want to focus on the first item (the representation of the program). I'll get to that in the next section, but first it helps to see the behaviour of a game.

The authors published the source code they used, so I used my laptop to run it (the problem in part is one of throwing chip cycles at it, so my results are going to be limited). 

Here's an example for space invaders (you can stop after 25 seconds):

<div style="padding:131.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/296395700?byline=0&portrait=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

What you're looking at there is the behaviour of an evolved program recorded playing a single game of Space Invaders via the Arcade Learning Environment (ALE).

To avoid confusion: this is what I achieved in one run on my laptop, and is not representative of the results reported in the paper. But even so, there's a hint there of something interesting happening with that first bullet dodge in the video, but otherwise it's pretty dumb. 

We can look at the program evolved for that game, and it looks like this:

![Diagram of inputs through compute nodes to output](/img/posts/2018-atari/space_invaders_0_zeros_removed.png)

What you're looking at is a graph of computations. At the top are the inputs, representing the red,  green and blue channels of a video frame.  The program doesn't make use of the third input.

At the bottom are the outputs. Space Invaders on the ALE platform has six controls: no-op, fire, left, right, left and fire, and right and fire. In our evolved programme only outputs 5 and 6 are used (left and right with firing).

Between them are compute nodes which are functions. They take as input the output from other functions or the video input. The node inputs and outputs can be matrices or single values.

For example, output 5 receives a value from the `atan` function (arctangent). That, in turn, takes input from a `first` and `add` function. What the functions do doesn't matter much, but we don't evolve them: they are the fundamental building blocks provided in the experiment.

You can trace the functions back until they reach an input.

To run the program you compute the output values, and the highest one is the winner and gets pushed.

# Representing a program

How does that program get represented and evolved? Looking at the encoding of that evolved program we see this:

```
[0.732719, 0.322254, 0.629519, 0.58417, 0.846816, 0.332693, 0.364999, 0.672248, 0.485399, 0.964311, 0.956811, 0.688574, 0.191743, 0.215527, 0.501322, 0.979395, 0.895049, 0.24854, 0.933796, 0.24931, 0.853642, 0.0184749, 0.0359777, 0.929975, 0.703265, 0.40679, 0.195708, 0.809868, 0.0519353, 0.413928, 0.130336, 0.134575, 0.513853, 0.136513, 0.741697, 0.456032, 0.742988, 0.679664, 0.485836, 0.958294, 0.69415, 0.358542, 0.0467613, 0.455783, 0.241988, 0.692082, 0.816422, 0.0401388, 0.947643, 0.361797, 0.83337, 0.740839, 0.775536, 0.0305614, 0.739933, 0.89206, 0.547986, 0.397794, 0.703992, 0.22079, 0.0248015, 0.0772889, 0.584075, 0.847732, 0.828782, 0.469608, 0.658981, 0.370057, 0.972892, 0.321443, 0.337078, 0.65886, 0.807534, 0.127399, 0.981925, 0.680182, 0.698415, 0.735522, 0.796834, 0.912534, 0.988334, 0.07669, 0.600324, 0.0159105, 0.219596, 0.268528, 0.437645, 0.0227603, 0.334503, 0.250675, 0.731421, 0.668472, 0.607447, 0.127101, 0.801624, 0.0380742, 0.0462373, 0.0455971, 0.0531209, 0.19359, 0.0235268, 0.524045, 0.636141, 0.173691, 0.986173, 0.340458, 0.127515, 0.713214, 0.804558, 0.422486, 0.603935, 0.542476, 0.920817, 0.547238, 0.683473, 0.21659, 0.155102, 0.725663, 0.753164, 0.0774076, 0.950964, 0.574776, 0.964115, 0.0782333, 0.87468, 0.443861, 0.371889, 0.954761, 0.0494845, 0.526747, 0.526706, 0.931708, 0.00195028, 0.658524, 0.833068, 0.273154, 0.0950734, 0.0107351, 0.180254, 0.974469, 0.757173, 0.514173, 0.0708619, 0.415589, 0.130095, 0.0780437, 0.398456, 0.848083, 0.11858, 0.160133, 0.638504, 0.122429, 0.592849, 0.440503, 0.157085, 0.347286, 0.40241, 0.61899, 0.932978, 0.823422, 0.567473, 0.122063, 0.887696, 0.872031, 0.609517, 0.924675, 0.939904, 0.0825759, 0.533575]
```

That's a 169 element array.

Cartesian Genetic Programming is so-called because it was initially a 2D grid of compute nodes. Each node had a _x_ and _y_ coordinate. That appears to have not been necessary, and now CGP uses a 1D array of nodes.

To make sense of the 169 values we have to understand the structure. It's like this:

![Diagram showing the structure of the chromosome](/img/posts/2018-atari/sketch.jpeg)

The first three values represent the inputs. The system does not evolve the inputs, and from our perspective they exist as targets for other nodes to point at. Remember that we're building up a graph, and nodes in the graph need to reference other nodes.

The next batch of values represent the outputs. A value here is a pointer towards a compute note. To draw the graph you take an output node, see what it points to, and then walk the graph.  These values are evolved, meaning what an output points to can change throughout evolution.

That's 9 of the 169 values covered. What follows next is 40 (in this example) compute nodes each made up of 4 values. That's 160 values, getting us to the 169 total.

Those four values control which function the node represents and what inputs it takes. Every function takes a third value as input, which is _p_ in the diagram above. Many functions ignore the value, but it is used sometimes, such as a range function.

Each one of those four values is a node (or "gene"). We evolve these values.

I found it helpful to trace an output to see how the graph builds up. So let's take a look at how we get to `atan` connect to `out5`.

Output 5 will be the 8th value in our array: we skip the three inputs then count five forward. It has the value 0.672248. This number is a reference to a compute node. We scale the value to the number of nodes we have (43, including the inputs):

```
julia> node_index = Int64(ceil(0.672248 * 43))
29
```

The authors of the paper used a language called Julia, and that's what I'm using in this post for any code.

So we know we need to see what compute node 29 is all about. If we reshape our input array into groups of 4 we can find that out pretty easily:

```
# b is the 169-value array
rgenes = reshape(b[(nin+nout+1):end], (4, num_nodes))'
40Ã—4 Array{Float64,2}:
```

The four values for node 29 are:

```
julia> rgenes[29,:]
4-element Array{Float64,1}:
 0.574776 
 0.964115 
 0.0782333
 0.87468 
 ```
 
 The last one of those numbers is the function (the 1st is the extra constant input parameter; the 2nd and 3rd are pointers to other nodes to use as input). We have a big list of functions, and we scale 0.87468 into that list and find the function is `f_atan`. 
 
Next we need to find out which other nodes are input to node 29. That's similar to how we found node 29, but there's a complication. In CGP there's a control, called _r_ (recurrent connections) which controls how far back connection can be made in the graph (or: looking at it another way, how far forward can you reference). If _r_ is zero, you can only receive inputs from nodes before you. If _t_ is 1, you can receive inputs from anywhere. For this experiment _r_ was 0.1.

OK, that's probably too much detail. But node 29 requires nodes 18 and 16 as input. And those nodes are `add` and `first`. And we can continue on until we find references back to inputs.

# Tricks

There's more to it than that, and I don't fully understand everything in the CGP software yet. But one thing I was wondering was: where's outputs 1, 2, 3 and 4 on the diagram?

I can imagine trimming out long graphs of nodes. Or nodes that lead to no inputs. But there's another trick. The figure I showed above, with just output 5 and 6 on it, is only showing outputs used during a game. That is: count how many times an output is triggered, and if it's never triggered, don't draw the node.

If we draw everything back from all outputs, the program looks like this:

![Full diagram of inputs through compute nodes to output](/img/posts/2018-atari/space_invaders_full_0.png)

# Evolution

With all of that in hand, we can see how a population of floating point numbers can represent programs. Evaluating those programs by playing a game allows us to score them. We can then produce new offspring by mutating the best players. And round we go until we've had enough.

# Context: why is this interesting?

In 2015 Nature published a paper from DeepMind which showed deep-reinforcement learning used to play Atari games. That is, they used deep neural networks fed with video frames and the game score as it played. It was a big deal: it achieved a level comparable with professional human gamers and didn't require tweaking for each game. It set the stage for the tacking the game of Go.

That's one algorithm (a "deep Q-network"), but as we've seen, other algorithms are available. The above work, published in the Genetic and Evolutionary Computation Conference, reported on a genetic algorithm that can. That is, evolving programmes to play Atari games. "While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark
set and require less training time."

I find that interesting because simple programmes might be things we can understand. In comparison, the solutions produced by deep Q-networks are opaque.


# References

TODO

## Papers

Wilson et al (2018) 
Evolving simple programs for playing Atari games
arXiv:1806.05695v1 (PDF)
https://arxiv.org/pdf/1806.05695.pdf

Mnih et al (2015) Human-level control through Deep Reinforcement Learning
doi:10.1038/nature14236 (PDF)
https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf


Silver et al. (2016) 
Mastering the game of Go with deep neural networks and tree search
doi:10.1038/nature16961
https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf

## Code

Julia

https://github.com/mgbellemare/Arcade-Learning-Environment


